{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29728a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.ipynb\n",
    "%run constrained_distributions.ipynb\n",
    "%run processors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee25e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal, multinomial, invwishart, rv_continuous\n",
    "from scipy.linalg import sqrtm, inv\n",
    "from numpy.random import uniform, dirichlet\n",
    "from numpy.linalg import slogdet\n",
    "from copy import deepcopy\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56f050d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gibbs_update(dat, param, hyp, alpha, labels):\n",
    "    NIW = [x[0] for x in param]\n",
    "    prop = [x[1] for x in param]\n",
    "    z = [x[2] for x in param]\n",
    "\n",
    "    kappa0, mu0, Psi0 = hyp\n",
    "\n",
    "    nw = len(prop)\n",
    "    nm = len(prop[0])\n",
    "    n, dm = np.shape(dat)\n",
    "\n",
    "    # Sample new mean estimates from constrained MVN distribution in each cluster\n",
    "    NIW_out = np.copy(NIW)\n",
    "    for i in range(nw):\n",
    "        rles = np.array(np.unique(np.sort(z[i]), return_counts=True))\n",
    "        nz = np.zeros(nm)\n",
    "        nz[rles[0]-1] = rles[1]\n",
    "\n",
    "        # xbar is a list of dm dictionaries\n",
    "        xbar = dat.groupby(z[0]).mean()\n",
    "        for m in range(nm):\n",
    "            if nz[m] >= dm:\n",
    "                # Compute this only once because it gets reused a lot here\n",
    "                xbarmap = [xbar[x][m+1] for x in xbar]\n",
    "\n",
    "                # Extract the current estimate for Sigma for this cluster\n",
    "                Sigma_hat = NIW[i][m].get(\"Sigma\", 0)\n",
    "\n",
    "                # Pre-compute some inverses we use multiple times\n",
    "                iS0 = np.linalg.inv(Sigma_hat / max(kappa0[m], dm))\n",
    "                iS = np.linalg.inv(Sigma_hat)\n",
    "\n",
    "                # Check if we are ok for mu\n",
    "                likelihood_check = np.full(dm, False)\n",
    "                rcMVN_in = np.dot(np.linalg.inv(iS0 + nz[m] * iS), (iS0.dot(mu0[m, :]) + nz[m] * iS.dot(xbarmap)))\n",
    "\n",
    "                for j in range(dm):\n",
    "                    if np.sign(rcMVN_in[j]) != np.sign(labels[m][j]) and np.sign(labels[m][j]) != 0:\n",
    "                        likelihood_check[j] = True\n",
    "\n",
    "                if np.any(likelihood_check):\n",
    "                    # Sample from the prior for mu\n",
    "                    mu = np.random.multivariate_normal(mu0[m, :], np.round(Sigma_hat, decimals=8), 1)\n",
    "                else:\n",
    "                    mu = np.random.multivariate_normal(np.linalg.inv(iS0 + nz[m] * iS).dot(rcMVN_in), np.linalg.inv(iS0 + nz[m] * iS), 1)\n",
    "\n",
    "                NIW_out[i][m][\"mu\"] = mu[0]\n",
    "            else:\n",
    "                # Draw from the prior\n",
    "                Sigma_hat = NIW[i][m].get(\"Sigma\", 0)\n",
    "                mu = np.random.multivariate_normal(mu0[m, :], np.round(Sigma_hat, decimals=8), 1)\n",
    "                NIW_out[i][m][\"mu\"] = mu[0]\n",
    "\n",
    "    # Sample new cluster labels\n",
    "    zout = np.copy(z)\n",
    "    for i in range(nw):\n",
    "        distns = [multivariate_normal(NIW_out[i][m][\"mu\"], np.round(NIW_out[i][m][\"Sigma\"], decimals=8)) for m in range(nm)]\n",
    "        p = np.zeros((n, nm))\n",
    "        for m in range(nm):\n",
    "            p[:, m] = distns[m].pdf(dat) * prop[i][m]\n",
    "\n",
    "        zout[i] = np.apply_along_axis(lambda x: np.random.choice(np.arange(nm), p=x / np.sum(x)), axis=1, arr=p)\n",
    "\n",
    "    # Sample new cluster weights\n",
    "    propout = np.copy(prop)\n",
    "    for i in range(nw):\n",
    "        rles = np.unique(zout[i], return_counts=True)\n",
    "        nz = np.zeros(nm)\n",
    "        nz[rles[0]] = rles[1]\n",
    "        propout[i] = np.random.dirichlet(np.add(alpha,nz))\n",
    "\n",
    "    return zout, NIW_out, propout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7ccfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(dat, param, nz, walker_num, cluster_num, curr_Sigma, Sigma_star, mu_hat):\n",
    "    NIW = [x[0] for x in param]\n",
    "    prop = [x[1] for x in param]\n",
    "    z = [x[2] for x in param]\n",
    "\n",
    "    n, dm = np.shape(dat)\n",
    "    nm = np.shape(prop[walker_num])[0]\n",
    "    subdat = dat[np.array(z[walker_num]) == cluster_num + 1].values\n",
    "\n",
    "    # If there are enough observations in the cluster, compute the log difference of the likelihoods\n",
    "    if nz >= dm:\n",
    "        mvn_distn_star = multivariate_normal(mu_hat, Sigma_star)\n",
    "        mvn_distn_curr = multivariate_normal(mu_hat, curr_Sigma)\n",
    "        ll_normal = np.sum(mvn_distn_star.logpdf(subdat)) - np.sum(mvn_distn_curr.logpdf(subdat))\n",
    "    else:\n",
    "        ll_normal = 0.0\n",
    "\n",
    "    # Establish the multivariate distributions that describe the mixture\n",
    "    distns_curr = [multivariate_normal(np.array(x['mu']), np.array(x[\"Sigma\"])) for x in NIW[walker_num]]\n",
    "    distns_star = distns_curr.copy()\n",
    "    distns_star[cluster_num] = multivariate_normal(np.array(NIW[walker_num][cluster_num][\"mu\"]), Sigma_star)\n",
    "\n",
    "    # Compute density of the data in each mixture component\n",
    "    p_curr = np.empty((n, nm))\n",
    "    for m in range(nm):\n",
    "        p_curr[:, m] = distns_curr[m].pdf(dat) * prop[walker_num][m]\n",
    "\n",
    "    p_star = p_curr.copy()\n",
    "    p_star[:, cluster_num] = distns_star[cluster_num].pdf(dat) * prop[walker_num][cluster_num]\n",
    "\n",
    "    # Convert density to probabilities\n",
    "    prob_curr = np.apply_along_axis(lambda x: x / np.sum(x) if not np.all(x == 0.0) else np.ones_like(x),\n",
    "                                     axis=1, arr=p_curr)\n",
    "    prob_star = np.apply_along_axis(lambda x: x / np.sum(x) if not np.all(x == 0.0) else np.ones_like(x),\n",
    "                                     axis=1, arr=p_star)\n",
    "\n",
    "    # Plug probabilities into the multinomial likelihood\n",
    "    mult_distns_curr = [multinomial(1, x) for x in prob_curr]\n",
    "    ll_mult_curr = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        occ = np.zeros(nm, dtype=int)\n",
    "        occ[z[walker_num][i]-1] = 1\n",
    "        ll_mult_curr[i] = mult_distns_curr[i].logpmf(occ)\n",
    "\n",
    "    mult_distns_star = [multinomial(1, x) for x in prob_star]\n",
    "    ll_mult_star = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        occ = np.zeros(nm, dtype=int)\n",
    "        occ[z[walker_num][i]-1] = 1\n",
    "        ll_mult_star[i] = mult_distns_star[i].logpmf(occ)\n",
    "\n",
    "    sum1 = np.sum(ll_mult_star)\n",
    "    sum2 = np.sum(ll_mult_curr)\n",
    "    if np.isnan(sum1 - sum2):\n",
    "        return ll_normal\n",
    "    else:\n",
    "        return sum1 - sum2 + ll_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3279718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(curr_Sigma, Sigma_star, mu_hat, hyp, cluster_num):\n",
    "    \"\"\"\n",
    "    The prior is based on two main parts\n",
    "        1. the density of curr_Sigma and Sigma_star given the prior on Sigma\n",
    "        2. the density of the current estimate of mu given the MVNormal\n",
    "            parameterized by either curr_Sigma or Sigma_star\n",
    "    \"\"\"\n",
    "    kappa0, mu0, Psi0 = hyp\n",
    "    dm = curr_Sigma.shape[0]\n",
    "    mnz = max(kappa0[cluster_num], dm + 2)\n",
    "\n",
    "    norm_distn_curr = multivariate_normal(mean=mu0[cluster_num,:], cov=curr_Sigma)\n",
    "    norm_distn_star = multivariate_normal(mean=mu0[cluster_num,:], cov=Sigma_star)\n",
    "    invwish_distn = invwishart(df=mnz, scale=Psi0[:,:,cluster_num])\n",
    "\n",
    "    linvwish_ratio = invwish_distn.logpdf(Sigma_star / (mnz - dm - 1)) - invwish_distn.logpdf(curr_Sigma / (mnz - dm - 1))\n",
    "    lnorm_ratio = norm_distn_star.logpdf(mu_hat) - norm_distn_curr.logpdf(mu_hat)\n",
    "\n",
    "    return linvwish_ratio + lnorm_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca9bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lhastings(curr_Sigma, Sigma_star, tune_df):\n",
    "    \"\"\"\n",
    "    The ratio of two Wishart distributions (all indicators cancel out)\n",
    "    \"\"\"\n",
    "    dm = Sigma_star.shape[0]\n",
    "\n",
    "    term1 = ((2*tune_df-dm-1) / 2) * (slogdet(curr_Sigma)[1] - slogdet(Sigma_star)[1])\n",
    "    term2 = (np.trace(inv(curr_Sigma).dot(Sigma_star)) - np.trace(inv(Sigma_star).dot(curr_Sigma))) * tune_df / 2\n",
    "\n",
    "    return term1 + term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "001f90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_Sigma(curr_Sigma, lab, tune_df):\n",
    "    \"\"\"\n",
    "    Draw a covariance from the constrained Wishart distribution\n",
    "    \"\"\"\n",
    "    return rand_constrained_Wish(curr_Sigma, tune_df, lab) / tune_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f128b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mcmc_move(dat, param, hyp, alpha, labels, tune_df):\n",
    "    \"\"\"\n",
    "    Make proposals cluster-at-a-time\n",
    "    For current parameter estimates and current cluster,\n",
    "    propose a new covariance according to the constrained Wishart\n",
    "    Return acpt, which tracks acceptances in each cluster\n",
    "    (1 => accept, 0 => reject)\n",
    "    \"\"\"\n",
    "    NIW = [x[0] for x in param]\n",
    "    prop = [x[1] for x in param]\n",
    "    z = [x[2] for x in param]\n",
    "    kappa0, mu0, Psi0 = hyp\n",
    "\n",
    "    nm = np.array(prop[0]).shape[0]\n",
    "    nw = len(prop)\n",
    "\n",
    "    # Tracking the acceptance rate\n",
    "    acpt = np.zeros(nm)\n",
    "\n",
    "    # For each walker, for each cluster, sample a covariance from the cWISH\n",
    "    for i in range(nw):\n",
    "        rles = np.array(np.unique(np.sort(z[0]), return_counts=True))\n",
    "        nz = np.zeros(nm)\n",
    "        nz[rles[0]-1] = rles[1]\n",
    "\n",
    "        mu_hats = [x['mu'] for x in NIW[i]]\n",
    "        curr_Sigmas = [x['Sigma'] for x in NIW[i]]\n",
    "        for m in range(nm):\n",
    "            \n",
    "            # Draw Sigma star from constrained Wishart distribution\n",
    "            Sigma_star = propose_Sigma(curr_Sigmas[m], labels[m], tune_df[m])\n",
    "            lhaste = get_lhastings(curr_Sigmas[m], Sigma_star, tune_df[m])\n",
    "\n",
    "            # Compute the log prior for this proposal - log prior for the current estimate\n",
    "            lp = logprior(curr_Sigmas[m], Sigma_star, mu_hats[m], hyp, m)\n",
    "\n",
    "            # Compute the log likelihood for this proposal - log likelihood for the current estimate\n",
    "            ll = log_likelihood(dat, param, nz[m], i, m, curr_Sigmas[m], Sigma_star, mu_hats[m])\n",
    "\n",
    "            # If random uniform small enough, update Sigma to Sigma_star\n",
    "            if np.log(np.random.uniform(0, 1)) < (ll + lp + lhaste):\n",
    "                NIW[i][m][\"Sigma\"] = Sigma_star\n",
    "                acpt[m] = 1\n",
    "\n",
    "    return (NIW, acpt) #tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "065c2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc(dat, param, hyp, alpha, nstep, labels, tune_df, opt_rate=0.3):\n",
    "    \"\"\"\n",
    "    The function will return `(chain, acpt_chain, tune_df_chain)` where\n",
    "    each returned chain value has an extra dimension appended counting steps of the\n",
    "    chain (so `chain` is of shape `(ndim, nwalkers, nstep)`, for example).\n",
    "    * acpt_chain tracks the acceptance rate for each cluster across the chain\n",
    "    * tune_df_chain tracks the tuning degrees of freedom in the wishart proposal across the chain\n",
    "    * dat is an n x nd array of observations\n",
    "    * alpha is an nm array of hyperparameters for the mixing proportions\n",
    "    * nstep = the number of steps of the already tuned mcmc\n",
    "    * nd = number of dimensions\n",
    "    * nw = number of walkers\n",
    "    * param contains current parameter estimates for each dimension\n",
    "        across walkers\n",
    "    \"\"\"\n",
    "\n",
    "    labels = [[int(digit) - 1 for digit in x] for x in labels] #list(13)\n",
    "\n",
    "    nw = len(param)\n",
    "    n, nd = dat.shape\n",
    "    nm = param[0][1].shape[0]\n",
    "    \n",
    "    chain = [[None]]\n",
    "    for j in range(nw):\n",
    "        chain[j][0] = copy.deepcopy(param[j])\n",
    "\n",
    "    acpt_tracker = np.zeros(nm)\n",
    "    \n",
    "    # For tracking the acceptance rate, per cluster, for the adaptive tuning variance\n",
    "    win_len = min(nstep, 50)\n",
    "    acpt_win = np.zeros((nm, win_len))\n",
    "    acpt_chain = np.zeros((nm, nstep))\n",
    "    \n",
    "    old_tune_df = copy.deepcopy(tune_df)\n",
    "    tune_df_chain = np.zeros((nm, nstep))\n",
    "\n",
    "    for i in range(nstep):\n",
    "        # Proposes IW cluster at a time, accepts/rejects/returns new IW\n",
    "        NIW, acpt = make_mcmc_move(dat, chain[i], hyp, alpha, labels, tune_df)\n",
    "        acpt_tracker += acpt\n",
    "        acpt_win[:, i % win_len] = acpt\n",
    "\n",
    "        # Makes Gibbs updates of means, mixing weights, and class labels\n",
    "        newz, newNIW, newprop = make_gibbs_update(dat, chain[i], hyp, alpha, labels)\n",
    "\n",
    "        if i > 49:\n",
    "            # Update tuning parameter per cluster\n",
    "            gamma1 = 10 / (i ** 0.8)\n",
    "            old_tune_df = deepcopy(tune_df)\n",
    "            for m in range(nm):\n",
    "                tune_df[m] = update_tune_df(tune_df[m], np.mean(acpt_win[m,:]), opt_rate, gamma1)\n",
    "\n",
    "        for j in range(nw):\n",
    "            chain_link = copy.deepcopy(param)\n",
    "\n",
    "            chain_link[j][0] = newNIW[j].tolist()\n",
    "            chain_link[j][1] = newprop[j]\n",
    "            chain_link[j][2] = newz[j] + 1\n",
    "\n",
    "            chain.append(chain_link)\n",
    "            acpt_chain[:, i] = acpt_tracker / (i + 1)\n",
    "            tune_df_chain[:, i] = tune_df\n",
    "\n",
    "    return chain, acpt_chain, tune_df_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
